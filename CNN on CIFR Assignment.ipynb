{"cells":[{"cell_type":"markdown","metadata":{"id":"kK3alCdFflQX"},"source":["### CNN on CIFR Assignment:"]},{"cell_type":"markdown","metadata":{"id":"cHCYMwwXflQd"},"source":["1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n","2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n","3.  You cannot use DropOut layers.\n","4.  You MUST use Image Augmentation Techniques.\n","5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n","6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n","7.  You cannot use test images for training the model.\n","8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n","9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n","10. You cannot have more than 1 Million parameters in total\n","11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n","12. You can use any optimization algorithm you need. \n","13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TLVcyNYKflQi"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import models, layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Hyperparameters\n","batch_size = 128\n","num_classes = 10\n","epochs = 10\n","l = 40\n","num_filter = 12\n","compression = 0.5\n","dropout_rate = 0.2"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Load CIFAR10 Data\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n","\n","# convert to one hot encoing \n","y_train = tf.keras.utils.to_categorical(y_train, 10)\n","y_test = tf.keras.utils.to_categorical(y_test, 10) "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Normalizing  (0 to 1)\n","# https://stackoverflow.com/questions/62783984/how-to-normalize-pixel-values-in-an-image-and-save-it\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","X_train /= 255\n","X_test /= 255\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def model_train(X_train, y_train, X_test, y_test, step_size, epochs):\n","    # Image Augmentation\n","    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n","    iterator_train = datagen.flow(X_train, y_train, batch_size=64)\n","\n","    # fit model\n","    steps = int(X_train.shape[0] / step_size)\n","    history = model.fit(iterator_train, steps_per_epoch=steps, epochs=epochs, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)\n","    \n","    # evaluate model\n","    print(model.evaluate(X_test, y_test, verbose=1))\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Dense Block\n","def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    temp = input\n","    for _ in range(l): \n","        BatchNorm = layers.BatchNormalization()(temp)\n","        relu = layers.Activation('relu')(BatchNorm)\n","        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n","        if dropout_rate>0:\n","            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n","        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n","        \n","        temp = concat\n","        \n","    return temp\n","\n","## transition Blosck\n","def transition(input, num_filter = 12, dropout_rate = 0.2):\n","    global compression\n","    BatchNorm = layers.BatchNormalization()(input)\n","    relu = layers.Activation('relu')(BatchNorm)\n","    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n","    if dropout_rate>0:\n","         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n","    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n","    return avg\n","\n","#output layer\n","def output_layer(input):\n","    global compression\n","    BatchNorm = layers.BatchNormalization()(input)\n","    relu = layers.Activation('relu')(BatchNorm)\n","    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n","    flat = layers.Flatten()(AvgPooling)\n","    output = layers.Dense(num_classes, activation='softmax')(flat)\n","    return output"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["num_filter = 12\n","dropout_rate = 0\n","l = 12\n","input = layers.Input(shape=(img_height, img_width, channel,))\n","First_Conv2D = layers.Conv2D(356, (3,3), use_bias=False ,padding='same')(input)\n","\n","First_Block = denseblock(First_Conv2D, 10, dropout_rate)\n","First_Transition = transition(First_Block, 128, dropout_rate)\n","\n","Second_Block = denseblock(First_Transition, 10, dropout_rate)\n","Second_Transition = transition(Second_Block, 128, dropout_rate)\n","\n","Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n","Third_Transition = transition(Third_Block, 64, dropout_rate)\n","\n","Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n","output = output_layer(Last_Block)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 32, 32, 356)  9612        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 32, 32, 356)  1424        conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 32, 32, 356)  0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 5)    16020       activation[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 32, 32, 361)  0           conv2d[0][0]                     \n","                                                                 conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 361)  1444        concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 361)  0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 5)    16245       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 32, 32, 366)  0           concatenate[0][0]                \n","                                                                 conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 366)  1464        concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 366)  0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 5)    16470       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 32, 32, 371)  0           concatenate_1[0][0]              \n","                                                                 conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 371)  1484        concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 371)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 5)    16695       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 32, 376)  0           concatenate_2[0][0]              \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 32, 32, 376)  1504        concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 32, 32, 376)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 5)    16920       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 32, 32, 381)  0           concatenate_3[0][0]              \n","                                                                 conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 32, 32, 381)  1524        concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 32, 32, 381)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 5)    17145       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 32, 32, 386)  0           concatenate_4[0][0]              \n","                                                                 conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 32, 32, 386)  1544        concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 32, 32, 386)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 5)    17370       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 32, 32, 391)  0           concatenate_5[0][0]              \n","                                                                 conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 32, 32, 391)  1564        concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 32, 32, 391)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 32, 32, 5)    17595       activation_7[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_7 (Concatenate)     (None, 32, 32, 396)  0           concatenate_6[0][0]              \n","                                                                 conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 32, 32, 396)  1584        concatenate_7[0][0]              \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 32, 32, 396)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 5)    17820       activation_8[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_8 (Concatenate)     (None, 32, 32, 401)  0           concatenate_7[0][0]              \n","                                                                 conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 32, 401)  1604        concatenate_8[0][0]              \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 32, 32, 401)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 5)    18045       activation_9[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 32, 32, 406)  0           concatenate_8[0][0]              \n","                                                                 conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 32, 406)  1624        concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 32, 32, 406)  0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 5)    18270       activation_10[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 32, 32, 411)  0           concatenate_9[0][0]              \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 32, 32, 411)  1644        concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 32, 32, 411)  0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 32, 32, 5)    18495       activation_11[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_11 (Concatenate)    (None, 32, 32, 416)  0           concatenate_10[0][0]             \n","                                                                 conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 32, 32, 416)  1664        concatenate_11[0][0]             \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 32, 32, 416)  0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 32, 32, 64)   239616      activation_12[0][0]              \n","__________________________________________________________________________________________________\n","average_pooling2d (AveragePooli (None, 16, 16, 64)   0           conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         average_pooling2d[0][0]          \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 16, 16, 5)    2880        activation_13[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 16, 16, 69)   0           average_pooling2d[0][0]          \n","                                                                 conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 16, 16, 69)   276         concatenate_12[0][0]             \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 16, 16, 69)   0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 16, 16, 5)    3105        activation_14[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_13 (Concatenate)    (None, 16, 16, 74)   0           concatenate_12[0][0]             \n","                                                                 conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 16, 16, 74)   296         concatenate_13[0][0]             \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 16, 16, 74)   0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 16, 16, 5)    3330        activation_15[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_14 (Concatenate)    (None, 16, 16, 79)   0           concatenate_13[0][0]             \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 16, 16, 79)   316         concatenate_14[0][0]             \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 16, 16, 79)   0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 5)    3555        activation_16[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_15 (Concatenate)    (None, 16, 16, 84)   0           concatenate_14[0][0]             \n","                                                                 conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 16, 16, 84)   336         concatenate_15[0][0]             \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 16, 16, 84)   0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 5)    3780        activation_17[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_16 (Concatenate)    (None, 16, 16, 89)   0           concatenate_15[0][0]             \n","                                                                 conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 89)   356         concatenate_16[0][0]             \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 16, 16, 89)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 16, 16, 5)    4005        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_17 (Concatenate)    (None, 16, 16, 94)   0           concatenate_16[0][0]             \n","                                                                 conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 94)   376         concatenate_17[0][0]             \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 16, 16, 94)   0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 16, 16, 5)    4230        activation_19[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_18 (Concatenate)    (None, 16, 16, 99)   0           concatenate_17[0][0]             \n","                                                                 conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 16, 16, 99)   396         concatenate_18[0][0]             \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 16, 16, 99)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 16, 16, 5)    4455        activation_20[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_19 (Concatenate)    (None, 16, 16, 104)  0           concatenate_18[0][0]             \n","                                                                 conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 16, 16, 104)  416         concatenate_19[0][0]             \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 16, 16, 104)  0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 16, 16, 5)    4680        activation_21[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 16, 16, 109)  0           concatenate_19[0][0]             \n","                                                                 conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 16, 16, 109)  436         concatenate_20[0][0]             \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 16, 16, 109)  0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 16, 16, 5)    4905        activation_22[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 16, 16, 114)  0           concatenate_20[0][0]             \n","                                                                 conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 16, 16, 114)  456         concatenate_21[0][0]             \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 16, 16, 114)  0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 16, 16, 5)    5130        activation_23[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 16, 16, 119)  0           concatenate_21[0][0]             \n","                                                                 conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 16, 16, 119)  476         concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 16, 16, 119)  0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 16, 16, 5)    5355        activation_24[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_23 (Concatenate)    (None, 16, 16, 124)  0           concatenate_22[0][0]             \n","                                                                 conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 16, 16, 124)  496         concatenate_23[0][0]             \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 16, 16, 124)  0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 16, 16, 64)   71424       activation_25[0][0]              \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 8, 8, 64)     0           conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 8, 8, 64)     0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 8, 8, 6)      3456        activation_26[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_24 (Concatenate)    (None, 8, 8, 70)     0           average_pooling2d_1[0][0]        \n","                                                                 conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 8, 8, 70)     280         concatenate_24[0][0]             \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 8, 8, 70)     0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 8, 8, 6)      3780        activation_27[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_25 (Concatenate)    (None, 8, 8, 76)     0           concatenate_24[0][0]             \n","                                                                 conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 8, 8, 76)     304         concatenate_25[0][0]             \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 8, 8, 76)     0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 8, 8, 6)      4104        activation_28[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_26 (Concatenate)    (None, 8, 8, 82)     0           concatenate_25[0][0]             \n","                                                                 conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 8, 8, 82)     328         concatenate_26[0][0]             \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 8, 8, 82)     0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 8, 8, 6)      4428        activation_29[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_27 (Concatenate)    (None, 8, 8, 88)     0           concatenate_26[0][0]             \n","                                                                 conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 8, 8, 88)     352         concatenate_27[0][0]             \n","__________________________________________________________________________________________________\n","activation_30 (Activation)      (None, 8, 8, 88)     0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 8, 8, 6)      4752        activation_30[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_28 (Concatenate)    (None, 8, 8, 94)     0           concatenate_27[0][0]             \n","                                                                 conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 8, 8, 94)     376         concatenate_28[0][0]             \n","__________________________________________________________________________________________________\n","activation_31 (Activation)      (None, 8, 8, 94)     0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 8, 8, 6)      5076        activation_31[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_29 (Concatenate)    (None, 8, 8, 100)    0           concatenate_28[0][0]             \n","                                                                 conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 8, 8, 100)    400         concatenate_29[0][0]             \n","__________________________________________________________________________________________________\n","activation_32 (Activation)      (None, 8, 8, 100)    0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 8, 8, 6)      5400        activation_32[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_30 (Concatenate)    (None, 8, 8, 106)    0           concatenate_29[0][0]             \n","                                                                 conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 8, 8, 106)    424         concatenate_30[0][0]             \n","__________________________________________________________________________________________________\n","activation_33 (Activation)      (None, 8, 8, 106)    0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 8, 8, 6)      5724        activation_33[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_31 (Concatenate)    (None, 8, 8, 112)    0           concatenate_30[0][0]             \n","                                                                 conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 8, 8, 112)    448         concatenate_31[0][0]             \n","__________________________________________________________________________________________________\n","activation_34 (Activation)      (None, 8, 8, 112)    0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 8, 8, 6)      6048        activation_34[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_32 (Concatenate)    (None, 8, 8, 118)    0           concatenate_31[0][0]             \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 8, 8, 118)    472         concatenate_32[0][0]             \n","__________________________________________________________________________________________________\n","activation_35 (Activation)      (None, 8, 8, 118)    0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 8, 8, 6)      6372        activation_35[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_33 (Concatenate)    (None, 8, 8, 124)    0           concatenate_32[0][0]             \n","                                                                 conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 8, 8, 124)    496         concatenate_33[0][0]             \n","__________________________________________________________________________________________________\n","activation_36 (Activation)      (None, 8, 8, 124)    0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 8, 8, 6)      6696        activation_36[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_34 (Concatenate)    (None, 8, 8, 130)    0           concatenate_33[0][0]             \n","                                                                 conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 8, 8, 130)    520         concatenate_34[0][0]             \n","__________________________________________________________________________________________________\n","activation_37 (Activation)      (None, 8, 8, 130)    0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 8, 8, 6)      7020        activation_37[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_35 (Concatenate)    (None, 8, 8, 136)    0           concatenate_34[0][0]             \n","                                                                 conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 8, 8, 136)    544         concatenate_35[0][0]             \n","__________________________________________________________________________________________________\n","activation_38 (Activation)      (None, 8, 8, 136)    0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 8, 8, 32)     39168       activation_38[0][0]              \n","__________________________________________________________________________________________________\n","average_pooling2d_2 (AveragePoo (None, 4, 4, 32)     0           conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 4, 4, 32)     128         average_pooling2d_2[0][0]        \n","__________________________________________________________________________________________________\n","activation_39 (Activation)      (None, 4, 4, 32)     0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 4, 4, 6)      1728        activation_39[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_36 (Concatenate)    (None, 4, 4, 38)     0           average_pooling2d_2[0][0]        \n","                                                                 conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_40 (BatchNo (None, 4, 4, 38)     152         concatenate_36[0][0]             \n","__________________________________________________________________________________________________\n","activation_40 (Activation)      (None, 4, 4, 38)     0           batch_normalization_40[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 4, 4, 6)      2052        activation_40[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_37 (Concatenate)    (None, 4, 4, 44)     0           concatenate_36[0][0]             \n","                                                                 conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_41 (BatchNo (None, 4, 4, 44)     176         concatenate_37[0][0]             \n","__________________________________________________________________________________________________\n","activation_41 (Activation)      (None, 4, 4, 44)     0           batch_normalization_41[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 4, 4, 6)      2376        activation_41[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_38 (Concatenate)    (None, 4, 4, 50)     0           concatenate_37[0][0]             \n","                                                                 conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_42 (BatchNo (None, 4, 4, 50)     200         concatenate_38[0][0]             \n","__________________________________________________________________________________________________\n","activation_42 (Activation)      (None, 4, 4, 50)     0           batch_normalization_42[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 4, 4, 6)      2700        activation_42[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_39 (Concatenate)    (None, 4, 4, 56)     0           concatenate_38[0][0]             \n","                                                                 conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_43 (BatchNo (None, 4, 4, 56)     224         concatenate_39[0][0]             \n","__________________________________________________________________________________________________\n","activation_43 (Activation)      (None, 4, 4, 56)     0           batch_normalization_43[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 4, 4, 6)      3024        activation_43[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_40 (Concatenate)    (None, 4, 4, 62)     0           concatenate_39[0][0]             \n","                                                                 conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_44 (BatchNo (None, 4, 4, 62)     248         concatenate_40[0][0]             \n","__________________________________________________________________________________________________\n","activation_44 (Activation)      (None, 4, 4, 62)     0           batch_normalization_44[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 4, 4, 6)      3348        activation_44[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_41 (Concatenate)    (None, 4, 4, 68)     0           concatenate_40[0][0]             \n","                                                                 conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_45 (BatchNo (None, 4, 4, 68)     272         concatenate_41[0][0]             \n","__________________________________________________________________________________________________\n","activation_45 (Activation)      (None, 4, 4, 68)     0           batch_normalization_45[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 4, 4, 6)      3672        activation_45[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_42 (Concatenate)    (None, 4, 4, 74)     0           concatenate_41[0][0]             \n","                                                                 conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_46 (BatchNo (None, 4, 4, 74)     296         concatenate_42[0][0]             \n","__________________________________________________________________________________________________\n","activation_46 (Activation)      (None, 4, 4, 74)     0           batch_normalization_46[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 4, 4, 6)      3996        activation_46[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_43 (Concatenate)    (None, 4, 4, 80)     0           concatenate_42[0][0]             \n","                                                                 conv2d_47[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_47 (BatchNo (None, 4, 4, 80)     320         concatenate_43[0][0]             \n","__________________________________________________________________________________________________\n","activation_47 (Activation)      (None, 4, 4, 80)     0           batch_normalization_47[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 4, 4, 6)      4320        activation_47[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_44 (Concatenate)    (None, 4, 4, 86)     0           concatenate_43[0][0]             \n","                                                                 conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_48 (BatchNo (None, 4, 4, 86)     344         concatenate_44[0][0]             \n","__________________________________________________________________________________________________\n","activation_48 (Activation)      (None, 4, 4, 86)     0           batch_normalization_48[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 4, 4, 6)      4644        activation_48[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_45 (Concatenate)    (None, 4, 4, 92)     0           concatenate_44[0][0]             \n","                                                                 conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 4, 4, 92)     368         concatenate_45[0][0]             \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 4, 4, 92)     0           batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 4, 4, 6)      4968        activation_49[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_46 (Concatenate)    (None, 4, 4, 98)     0           concatenate_45[0][0]             \n","                                                                 conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_50 (BatchNo (None, 4, 4, 98)     392         concatenate_46[0][0]             \n","__________________________________________________________________________________________________\n","activation_50 (Activation)      (None, 4, 4, 98)     0           batch_normalization_50[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 4, 4, 6)      5292        activation_50[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_47 (Concatenate)    (None, 4, 4, 104)    0           concatenate_46[0][0]             \n","                                                                 conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_51 (BatchNo (None, 4, 4, 104)    416         concatenate_47[0][0]             \n","__________________________________________________________________________________________________\n","activation_51 (Activation)      (None, 4, 4, 104)    0           batch_normalization_51[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_3 (AveragePoo (None, 2, 2, 104)    0           activation_51[0][0]              \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 416)          0           average_pooling2d_3[0][0]        \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 10)           4170        flatten[0][0]                    \n","==================================================================================================\n","Total params: 759,162\n","Trainable params: 742,314\n","Non-trainable params: 16,848\n","__________________________________________________________________________________________________\n"]}],"source":["model = Model(inputs=[input], outputs=[output])\n","model.summary()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# determine Loss function and Optimizer\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["filepath = \"model.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"]},{"cell_type":"markdown","metadata":{},"source":["- Image Augmentation"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n","iterator_train = datagen.flow(X_train, y_train, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["- Training model for 20 Epochs"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","782/782 [==============================] - 318s 384ms/step - loss: 1.4224 - accuracy: 0.4794 - val_loss: 1.8568 - val_accuracy: 0.4414\n","\n","Epoch 00001: loss improved from inf to 1.42235, saving model to model.h5\n","Epoch 2/20\n","782/782 [==============================] - 287s 368ms/step - loss: 0.9651 - accuracy: 0.6592 - val_loss: 1.0305 - val_accuracy: 0.6485\n","\n","Epoch 00002: loss improved from 1.42235 to 0.96511, saving model to model.h5\n","Epoch 3/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.7723 - accuracy: 0.7270 - val_loss: 0.9682 - val_accuracy: 0.6667\n","\n","Epoch 00003: loss improved from 0.96511 to 0.77228, saving model to model.h5\n","Epoch 4/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.6674 - accuracy: 0.7648 - val_loss: 1.1553 - val_accuracy: 0.6281\n","\n","Epoch 00004: loss improved from 0.77228 to 0.66737, saving model to model.h5\n","Epoch 5/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.5977 - accuracy: 0.7924 - val_loss: 0.9137 - val_accuracy: 0.7038\n","\n","Epoch 00005: loss improved from 0.66737 to 0.59767, saving model to model.h5\n","Epoch 6/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.5433 - accuracy: 0.8137 - val_loss: 0.6819 - val_accuracy: 0.7749\n","\n","Epoch 00006: loss improved from 0.59767 to 0.54335, saving model to model.h5\n","Epoch 7/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.5050 - accuracy: 0.8258 - val_loss: 0.7380 - val_accuracy: 0.7563\n","\n","Epoch 00007: loss improved from 0.54335 to 0.50497, saving model to model.h5\n","Epoch 8/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.4700 - accuracy: 0.8358 - val_loss: 0.7104 - val_accuracy: 0.7769\n","\n","Epoch 00008: loss improved from 0.50497 to 0.46997, saving model to model.h5\n","Epoch 9/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.4437 - accuracy: 0.8474 - val_loss: 0.6443 - val_accuracy: 0.7898\n","\n","Epoch 00009: loss improved from 0.46997 to 0.44372, saving model to model.h5\n","Epoch 10/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.4165 - accuracy: 0.8568 - val_loss: 0.9546 - val_accuracy: 0.7102\n","\n","Epoch 00010: loss improved from 0.44372 to 0.41646, saving model to model.h5\n","Epoch 11/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3926 - accuracy: 0.8649 - val_loss: 0.5407 - val_accuracy: 0.8124\n","\n","Epoch 00011: loss improved from 0.41646 to 0.39258, saving model to model.h5\n","Epoch 12/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3780 - accuracy: 0.8689 - val_loss: 0.6980 - val_accuracy: 0.7779\n","\n","Epoch 00012: loss improved from 0.39258 to 0.37800, saving model to model.h5\n","Epoch 13/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3632 - accuracy: 0.8742 - val_loss: 0.7331 - val_accuracy: 0.7635\n","\n","Epoch 00013: loss improved from 0.37800 to 0.36319, saving model to model.h5\n","Epoch 14/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3433 - accuracy: 0.8805 - val_loss: 0.5335 - val_accuracy: 0.8190\n","\n","Epoch 00014: loss improved from 0.36319 to 0.34326, saving model to model.h5\n","Epoch 15/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3294 - accuracy: 0.8859 - val_loss: 0.4680 - val_accuracy: 0.8450\n","\n","Epoch 00015: loss improved from 0.34326 to 0.32938, saving model to model.h5\n","Epoch 16/20\n","782/782 [==============================] - 288s 368ms/step - loss: 0.3164 - accuracy: 0.8894 - val_loss: 0.5245 - val_accuracy: 0.8241\n","\n","Epoch 00016: loss improved from 0.32938 to 0.31643, saving model to model.h5\n","Epoch 17/20\n","782/782 [==============================] - 284s 363ms/step - loss: 0.3067 - accuracy: 0.8934 - val_loss: 0.4681 - val_accuracy: 0.8410\n","\n","Epoch 00017: loss improved from 0.31643 to 0.30675, saving model to model.h5\n","Epoch 18/20\n","782/782 [==============================] - 283s 362ms/step - loss: 0.2944 - accuracy: 0.8980 - val_loss: 0.4382 - val_accuracy: 0.8538\n","\n","Epoch 00018: loss improved from 0.30675 to 0.29441, saving model to model.h5\n","Epoch 19/20\n","782/782 [==============================] - 283s 362ms/step - loss: 0.2848 - accuracy: 0.9001 - val_loss: 0.4935 - val_accuracy: 0.8415\n","\n","Epoch 00019: loss improved from 0.29441 to 0.28484, saving model to model.h5\n","Epoch 20/20\n","782/782 [==============================] - 283s 362ms/step - loss: 0.2734 - accuracy: 0.9038 - val_loss: 0.4543 - val_accuracy: 0.8529\n","\n","Epoch 00020: loss improved from 0.28484 to 0.27343, saving model to model.h5\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x22fddc81190>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# fit model\n","#steps = int(X_train.shape[0] / 64)\n","model.fit(iterator_train, batch_size=256, epochs=20, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["- For 20 Epochs got Test accuracy of 85.29%."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 21s 59ms/step - loss: 0.4543 - accuracy: 0.8529\n","Test loss: 0.4542732536792755\n","Test accuracy: 0.8529000282287598\n"]}],"source":["# Test the model\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the last Trained model and continuing Training"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["model = load_model('model.h5')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# determine Loss function and Optimizer\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["#### Training for 200 Epochs (But early stopped at 102 Epoch)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","782/782 [==============================] - 319s 385ms/step - loss: 0.1369 - accuracy: 0.9514 - val_loss: 0.5026 - val_accuracy: 0.8670\n","\n","Epoch 00001: loss improved from inf to 0.13691, saving model to model.h5\n","Epoch 2/200\n","782/782 [==============================] - 289s 369ms/step - loss: 0.1384 - accuracy: 0.9500 - val_loss: 0.4711 - val_accuracy: 0.8732\n","\n","Epoch 00002: loss did not improve from 0.13691\n","Epoch 3/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.1328 - accuracy: 0.9528 - val_loss: 0.4438 - val_accuracy: 0.8752\n","\n","Epoch 00003: loss improved from 0.13691 to 0.13276, saving model to model.h5\n","Epoch 4/200\n","782/782 [==============================] - 267s 342ms/step - loss: 0.1277 - accuracy: 0.9543 - val_loss: 0.4569 - val_accuracy: 0.8773\n","\n","Epoch 00004: loss improved from 0.13276 to 0.12766, saving model to model.h5\n","Epoch 5/200\n","782/782 [==============================] - 271s 347ms/step - loss: 0.1250 - accuracy: 0.9546 - val_loss: 0.7220 - val_accuracy: 0.8247\n","\n","Epoch 00005: loss improved from 0.12766 to 0.12505, saving model to model.h5\n","Epoch 6/200\n","782/782 [==============================] - 287s 367ms/step - loss: 0.1265 - accuracy: 0.9548 - val_loss: 0.4118 - val_accuracy: 0.8838\n","\n","Epoch 00006: loss did not improve from 0.12505\n","Epoch 7/200\n","782/782 [==============================] - 288s 368ms/step - loss: 0.1235 - accuracy: 0.9562 - val_loss: 0.4673 - val_accuracy: 0.8741\n","\n","Epoch 00007: loss improved from 0.12505 to 0.12354, saving model to model.h5\n","Epoch 8/200\n","782/782 [==============================] - 275s 351ms/step - loss: 0.1199 - accuracy: 0.9558 - val_loss: 0.5044 - val_accuracy: 0.8588\n","\n","Epoch 00008: loss improved from 0.12354 to 0.11992, saving model to model.h5\n","Epoch 9/200\n","782/782 [==============================] - 273s 349ms/step - loss: 0.1193 - accuracy: 0.9568 - val_loss: 0.4682 - val_accuracy: 0.8744\n","\n","Epoch 00009: loss improved from 0.11992 to 0.11928, saving model to model.h5\n","Epoch 10/200\n","782/782 [==============================] - 267s 342ms/step - loss: 0.1137 - accuracy: 0.9590 - val_loss: 0.4015 - val_accuracy: 0.8835\n","\n","Epoch 00010: loss improved from 0.11928 to 0.11368, saving model to model.h5\n","Epoch 11/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.1126 - accuracy: 0.9591 - val_loss: 0.4038 - val_accuracy: 0.8847\n","\n","Epoch 00011: loss improved from 0.11368 to 0.11262, saving model to model.h5\n","Epoch 12/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.1110 - accuracy: 0.9607 - val_loss: 0.4609 - val_accuracy: 0.8752\n","\n","Epoch 00012: loss improved from 0.11262 to 0.11100, saving model to model.h5\n","Epoch 13/200\n","782/782 [==============================] - 271s 346ms/step - loss: 0.1116 - accuracy: 0.9595 - val_loss: 0.4437 - val_accuracy: 0.8726\n","\n","Epoch 00013: loss did not improve from 0.11100\n","Epoch 14/200\n","782/782 [==============================] - 275s 352ms/step - loss: 0.1084 - accuracy: 0.9614 - val_loss: 0.3991 - val_accuracy: 0.8893\n","\n","Epoch 00014: loss improved from 0.11100 to 0.10844, saving model to model.h5\n","Epoch 15/200\n","782/782 [==============================] - 277s 354ms/step - loss: 0.1082 - accuracy: 0.9608 - val_loss: 0.4594 - val_accuracy: 0.8771\n","\n","Epoch 00015: loss improved from 0.10844 to 0.10824, saving model to model.h5\n","Epoch 16/200\n","782/782 [==============================] - 277s 355ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.4315 - val_accuracy: 0.8786\n","\n","Epoch 00016: loss improved from 0.10824 to 0.10715, saving model to model.h5\n","Epoch 17/200\n","782/782 [==============================] - 276s 353ms/step - loss: 0.1018 - accuracy: 0.9636 - val_loss: 0.4507 - val_accuracy: 0.8803\n","\n","Epoch 00017: loss improved from 0.10715 to 0.10177, saving model to model.h5\n","Epoch 18/200\n","782/782 [==============================] - 292s 373ms/step - loss: 0.0987 - accuracy: 0.9642 - val_loss: 0.4534 - val_accuracy: 0.8827\n","\n","Epoch 00018: loss improved from 0.10177 to 0.09872, saving model to model.h5\n","Epoch 19/200\n","782/782 [==============================] - 267s 341ms/step - loss: 0.1026 - accuracy: 0.9632 - val_loss: 0.4430 - val_accuracy: 0.8757\n","\n","Epoch 00019: loss did not improve from 0.09872\n","Epoch 20/200\n","782/782 [==============================] - 270s 345ms/step - loss: 0.0977 - accuracy: 0.9643 - val_loss: 0.3951 - val_accuracy: 0.8961\n","\n","Epoch 00020: loss improved from 0.09872 to 0.09774, saving model to model.h5\n","Epoch 21/200\n","782/782 [==============================] - 279s 357ms/step - loss: 0.0991 - accuracy: 0.9645 - val_loss: 0.4334 - val_accuracy: 0.8902\n","\n","Epoch 00021: loss did not improve from 0.09774\n","Epoch 22/200\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0960 - accuracy: 0.9649 - val_loss: 0.4394 - val_accuracy: 0.8874\n","\n","Epoch 00022: loss improved from 0.09774 to 0.09601, saving model to model.h5\n","Epoch 23/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.0910 - accuracy: 0.9676 - val_loss: 0.4593 - val_accuracy: 0.8858\n","\n","Epoch 00023: loss improved from 0.09601 to 0.09104, saving model to model.h5\n","Epoch 24/200\n","782/782 [==============================] - 280s 358ms/step - loss: 0.0956 - accuracy: 0.9660 - val_loss: 0.4606 - val_accuracy: 0.8861\n","\n","Epoch 00024: loss did not improve from 0.09104\n","Epoch 25/200\n","782/782 [==============================] - 278s 355ms/step - loss: 0.0914 - accuracy: 0.9669 - val_loss: 0.5036 - val_accuracy: 0.8746\n","\n","Epoch 00025: loss did not improve from 0.09104\n","Epoch 26/200\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0950 - accuracy: 0.9655 - val_loss: 0.4390 - val_accuracy: 0.8847\n","\n","Epoch 00026: loss did not improve from 0.09104\n","Epoch 27/200\n","782/782 [==============================] - 269s 344ms/step - loss: 0.0916 - accuracy: 0.9672 - val_loss: 0.4737 - val_accuracy: 0.8862\n","\n","Epoch 00027: loss did not improve from 0.09104\n","Epoch 28/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0885 - accuracy: 0.9684 - val_loss: 0.4268 - val_accuracy: 0.8904\n","\n","Epoch 00028: loss improved from 0.09104 to 0.08846, saving model to model.h5\n","Epoch 29/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0853 - accuracy: 0.9699 - val_loss: 0.4912 - val_accuracy: 0.8787\n","\n","Epoch 00029: loss improved from 0.08846 to 0.08534, saving model to model.h5\n","Epoch 30/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0883 - accuracy: 0.9677 - val_loss: 0.4197 - val_accuracy: 0.8921\n","\n","Epoch 00030: loss did not improve from 0.08534\n","Epoch 31/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0825 - accuracy: 0.9701 - val_loss: 0.4149 - val_accuracy: 0.8965\n","\n","Epoch 00031: loss improved from 0.08534 to 0.08250, saving model to model.h5\n","Epoch 32/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0888 - accuracy: 0.9675 - val_loss: 0.4342 - val_accuracy: 0.8913\n","\n","Epoch 00032: loss did not improve from 0.08250\n","Epoch 33/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0836 - accuracy: 0.9699 - val_loss: 0.4376 - val_accuracy: 0.8911\n","\n","Epoch 00033: loss did not improve from 0.08250\n","Epoch 34/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0805 - accuracy: 0.9712 - val_loss: 0.4665 - val_accuracy: 0.8842\n","\n","Epoch 00034: loss improved from 0.08250 to 0.08047, saving model to model.h5\n","Epoch 35/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0816 - accuracy: 0.9707 - val_loss: 0.4598 - val_accuracy: 0.8869\n","\n","Epoch 00035: loss did not improve from 0.08047\n","Epoch 36/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0810 - accuracy: 0.9708 - val_loss: 0.4156 - val_accuracy: 0.8902\n","\n","Epoch 00036: loss did not improve from 0.08047\n","Epoch 37/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0828 - accuracy: 0.9704 - val_loss: 0.4430 - val_accuracy: 0.8925\n","\n","Epoch 00037: loss did not improve from 0.08047\n","Epoch 38/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0799 - accuracy: 0.9712 - val_loss: 0.4495 - val_accuracy: 0.8889\n","\n","Epoch 00038: loss improved from 0.08047 to 0.07988, saving model to model.h5\n","Epoch 39/200\n","782/782 [==============================] - 267s 341ms/step - loss: 0.0795 - accuracy: 0.9713 - val_loss: 0.4517 - val_accuracy: 0.8935\n","\n","Epoch 00039: loss improved from 0.07988 to 0.07954, saving model to model.h5\n","Epoch 40/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.0778 - accuracy: 0.9722 - val_loss: 0.4820 - val_accuracy: 0.8784\n","\n","Epoch 00040: loss improved from 0.07954 to 0.07783, saving model to model.h5\n","Epoch 41/200\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0773 - accuracy: 0.9722 - val_loss: 0.4914 - val_accuracy: 0.8859\n","\n","Epoch 00041: loss improved from 0.07783 to 0.07729, saving model to model.h5\n","Epoch 42/200\n","782/782 [==============================] - 279s 357ms/step - loss: 0.0710 - accuracy: 0.9742 - val_loss: 0.5180 - val_accuracy: 0.8847\n","\n","Epoch 00042: loss improved from 0.07729 to 0.07096, saving model to model.h5\n","Epoch 43/200\n","782/782 [==============================] - 290s 371ms/step - loss: 0.0773 - accuracy: 0.9722 - val_loss: 0.4677 - val_accuracy: 0.8889\n","\n","Epoch 00043: loss did not improve from 0.07096\n","Epoch 44/200\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0698 - accuracy: 0.9746 - val_loss: 0.4609 - val_accuracy: 0.8953\n","\n","Epoch 00044: loss improved from 0.07096 to 0.06977, saving model to model.h5\n","Epoch 45/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.4853 - val_accuracy: 0.8830\n","\n","Epoch 00045: loss did not improve from 0.06977\n","Epoch 46/200\n","782/782 [==============================] - 280s 358ms/step - loss: 0.0742 - accuracy: 0.9731 - val_loss: 0.4228 - val_accuracy: 0.8968\n","\n","Epoch 00046: loss did not improve from 0.06977\n","Epoch 47/200\n","782/782 [==============================] - 277s 354ms/step - loss: 0.0703 - accuracy: 0.9745 - val_loss: 0.5069 - val_accuracy: 0.8824\n","\n","Epoch 00047: loss did not improve from 0.06977\n","Epoch 48/200\n","782/782 [==============================] - 274s 350ms/step - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.5333 - val_accuracy: 0.8840\n","\n","Epoch 00048: loss did not improve from 0.06977\n","Epoch 49/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0699 - accuracy: 0.9746 - val_loss: 0.4360 - val_accuracy: 0.8952\n","\n","Epoch 00049: loss did not improve from 0.06977\n","Epoch 50/200\n","782/782 [==============================] - 275s 351ms/step - loss: 0.0658 - accuracy: 0.9763 - val_loss: 0.5081 - val_accuracy: 0.8783\n","\n","Epoch 00050: loss improved from 0.06977 to 0.06583, saving model to model.h5\n","Epoch 51/200\n","782/782 [==============================] - 276s 352ms/step - loss: 0.0669 - accuracy: 0.9757 - val_loss: 0.4838 - val_accuracy: 0.8886\n","\n","Epoch 00051: loss did not improve from 0.06583\n","Epoch 52/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0687 - accuracy: 0.9754 - val_loss: 0.4557 - val_accuracy: 0.8920\n","\n","Epoch 00052: loss did not improve from 0.06583\n","Epoch 53/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0653 - accuracy: 0.9770 - val_loss: 0.4629 - val_accuracy: 0.8936\n","\n","Epoch 00053: loss improved from 0.06583 to 0.06527, saving model to model.h5\n","Epoch 54/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0683 - accuracy: 0.9759 - val_loss: 0.4811 - val_accuracy: 0.8824\n","\n","Epoch 00054: loss did not improve from 0.06527\n","Epoch 55/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0649 - accuracy: 0.9767 - val_loss: 0.4690 - val_accuracy: 0.8884\n","\n","Epoch 00055: loss improved from 0.06527 to 0.06489, saving model to model.h5\n","Epoch 56/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0650 - accuracy: 0.9767 - val_loss: 0.5301 - val_accuracy: 0.8820\n","\n","Epoch 00056: loss did not improve from 0.06489\n","Epoch 57/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0655 - accuracy: 0.9768 - val_loss: 0.5356 - val_accuracy: 0.8807\n","\n","Epoch 00057: loss did not improve from 0.06489\n","Epoch 58/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0638 - accuracy: 0.9770 - val_loss: 0.4513 - val_accuracy: 0.8969\n","\n","Epoch 00058: loss improved from 0.06489 to 0.06380, saving model to model.h5\n","Epoch 59/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0607 - accuracy: 0.9778 - val_loss: 0.4821 - val_accuracy: 0.8939\n","\n","Epoch 00059: loss improved from 0.06380 to 0.06067, saving model to model.h5\n","Epoch 60/200\n","782/782 [==============================] - 266s 341ms/step - loss: 0.0605 - accuracy: 0.9776 - val_loss: 0.6060 - val_accuracy: 0.8757\n","\n","Epoch 00060: loss improved from 0.06067 to 0.06049, saving model to model.h5\n","Epoch 61/200\n","782/782 [==============================] - 269s 344ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.4804 - val_accuracy: 0.8943\n","\n","Epoch 00061: loss did not improve from 0.06049\n","Epoch 62/200\n","782/782 [==============================] - 275s 352ms/step - loss: 0.0651 - accuracy: 0.9771 - val_loss: 0.5165 - val_accuracy: 0.8846\n","\n","Epoch 00062: loss did not improve from 0.06049\n","Epoch 63/200\n","782/782 [==============================] - 276s 353ms/step - loss: 0.0635 - accuracy: 0.9770 - val_loss: 0.4441 - val_accuracy: 0.8981\n","\n","Epoch 00063: loss did not improve from 0.06049\n","Epoch 64/200\n","782/782 [==============================] - 288s 368ms/step - loss: 0.0590 - accuracy: 0.9784 - val_loss: 0.6054 - val_accuracy: 0.8773\n","\n","Epoch 00064: loss improved from 0.06049 to 0.05897, saving model to model.h5\n","Epoch 65/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.0584 - accuracy: 0.9793 - val_loss: 0.5091 - val_accuracy: 0.8893\n","\n","Epoch 00065: loss improved from 0.05897 to 0.05842, saving model to model.h5\n","Epoch 66/200\n","782/782 [==============================] - 281s 360ms/step - loss: 0.0590 - accuracy: 0.9787 - val_loss: 0.4628 - val_accuracy: 0.8950\n","\n","Epoch 00066: loss did not improve from 0.05842\n","Epoch 67/200\n","782/782 [==============================] - 280s 358ms/step - loss: 0.0597 - accuracy: 0.9786 - val_loss: 0.4372 - val_accuracy: 0.8964\n","\n","Epoch 00067: loss did not improve from 0.05842\n","Epoch 68/200\n","782/782 [==============================] - 280s 358ms/step - loss: 0.0598 - accuracy: 0.9783 - val_loss: 0.5694 - val_accuracy: 0.8797\n","\n","Epoch 00068: loss did not improve from 0.05842\n","Epoch 69/200\n","782/782 [==============================] - 280s 359ms/step - loss: 0.0585 - accuracy: 0.9793 - val_loss: 0.5012 - val_accuracy: 0.8898\n","\n","Epoch 00069: loss did not improve from 0.05842\n","Epoch 70/200\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0588 - accuracy: 0.9791 - val_loss: 0.4472 - val_accuracy: 0.8991\n","\n","Epoch 00070: loss did not improve from 0.05842\n","Epoch 71/200\n","782/782 [==============================] - 280s 359ms/step - loss: 0.0591 - accuracy: 0.9795 - val_loss: 0.4969 - val_accuracy: 0.8906\n","\n","Epoch 00071: loss did not improve from 0.05842\n","Epoch 72/200\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.4789 - val_accuracy: 0.8953\n","\n","Epoch 00072: loss improved from 0.05842 to 0.05616, saving model to model.h5\n","Epoch 73/200\n","782/782 [==============================] - 283s 362ms/step - loss: 0.0573 - accuracy: 0.9796 - val_loss: 0.4813 - val_accuracy: 0.8948\n","\n","Epoch 00073: loss did not improve from 0.05616\n","Epoch 74/200\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0575 - accuracy: 0.9793 - val_loss: 0.5576 - val_accuracy: 0.8880\n","\n","Epoch 00074: loss did not improve from 0.05616\n","Epoch 75/200\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0588 - accuracy: 0.9794 - val_loss: 0.5210 - val_accuracy: 0.8939\n","\n","Epoch 00075: loss did not improve from 0.05616\n","Epoch 76/200\n","782/782 [==============================] - 280s 359ms/step - loss: 0.0542 - accuracy: 0.9806 - val_loss: 0.4517 - val_accuracy: 0.8979\n","\n","Epoch 00076: loss improved from 0.05616 to 0.05415, saving model to model.h5\n","Epoch 77/200\n","782/782 [==============================] - 288s 369ms/step - loss: 0.0571 - accuracy: 0.9796 - val_loss: 0.4778 - val_accuracy: 0.8931\n","\n","Epoch 00077: loss did not improve from 0.05415\n","Epoch 78/200\n","782/782 [==============================] - 270s 345ms/step - loss: 0.0526 - accuracy: 0.9811 - val_loss: 0.4575 - val_accuracy: 0.8976\n","\n","Epoch 00078: loss improved from 0.05415 to 0.05261, saving model to model.h5\n","Epoch 79/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0530 - accuracy: 0.9810 - val_loss: 0.4789 - val_accuracy: 0.8940\n","\n","Epoch 00079: loss did not improve from 0.05261\n","Epoch 80/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0544 - accuracy: 0.9808 - val_loss: 0.5456 - val_accuracy: 0.8814\n","\n","Epoch 00080: loss did not improve from 0.05261\n","Epoch 81/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.5317 - val_accuracy: 0.8887\n","\n","Epoch 00081: loss did not improve from 0.05261\n","Epoch 82/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.5131 - val_accuracy: 0.8932\n","\n","Epoch 00082: loss did not improve from 0.05261\n","Epoch 83/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0527 - accuracy: 0.9815 - val_loss: 0.5244 - val_accuracy: 0.8932\n","\n","Epoch 00083: loss did not improve from 0.05261\n","Epoch 84/200\n","782/782 [==============================] - 267s 341ms/step - loss: 0.0522 - accuracy: 0.9820 - val_loss: 0.4657 - val_accuracy: 0.8967\n","\n","Epoch 00084: loss improved from 0.05261 to 0.05220, saving model to model.h5\n","Epoch 85/200\n","782/782 [==============================] - 285s 365ms/step - loss: 0.0554 - accuracy: 0.9800 - val_loss: 0.6151 - val_accuracy: 0.8741\n","\n","Epoch 00085: loss did not improve from 0.05220\n","Epoch 86/200\n","782/782 [==============================] - 283s 362ms/step - loss: 0.0507 - accuracy: 0.9821 - val_loss: 0.5317 - val_accuracy: 0.8885\n","\n","Epoch 00086: loss improved from 0.05220 to 0.05070, saving model to model.h5\n","Epoch 87/200\n","782/782 [==============================] - 283s 362ms/step - loss: 0.0510 - accuracy: 0.9819 - val_loss: 0.4822 - val_accuracy: 0.8990\n","\n","Epoch 00087: loss did not improve from 0.05070\n","Epoch 88/200\n","782/782 [==============================] - 273s 350ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.4750 - val_accuracy: 0.8981\n","\n","Epoch 00088: loss did not improve from 0.05070\n","Epoch 89/200\n","782/782 [==============================] - 273s 349ms/step - loss: 0.0530 - accuracy: 0.9813 - val_loss: 0.4694 - val_accuracy: 0.9006\n","\n","Epoch 00089: loss did not improve from 0.05070\n","Epoch 90/200\n","782/782 [==============================] - 272s 348ms/step - loss: 0.0502 - accuracy: 0.9822 - val_loss: 0.5346 - val_accuracy: 0.8889\n","\n","Epoch 00090: loss improved from 0.05070 to 0.05016, saving model to model.h5\n","Epoch 91/200\n","782/782 [==============================] - 274s 350ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.5014 - val_accuracy: 0.8918\n","\n","Epoch 00091: loss did not improve from 0.05016\n","Epoch 92/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.4695 - val_accuracy: 0.8999\n","\n","Epoch 00092: loss did not improve from 0.05016\n","Epoch 93/200\n","782/782 [==============================] - 280s 359ms/step - loss: 0.0559 - accuracy: 0.9805 - val_loss: 0.5064 - val_accuracy: 0.8923\n","\n","Epoch 00093: loss did not improve from 0.05016\n","Epoch 94/200\n","782/782 [==============================] - 282s 361ms/step - loss: 0.0511 - accuracy: 0.9823 - val_loss: 0.4818 - val_accuracy: 0.8954\n","\n","Epoch 00094: loss did not improve from 0.05016\n","Epoch 95/200\n","782/782 [==============================] - 274s 350ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 0.6220 - val_accuracy: 0.8765\n","\n","Epoch 00095: loss improved from 0.05016 to 0.04403, saving model to model.h5\n","Epoch 96/200\n","782/782 [==============================] - 267s 341ms/step - loss: 0.0482 - accuracy: 0.9825 - val_loss: 0.4546 - val_accuracy: 0.8968\n","\n","Epoch 00096: loss did not improve from 0.04403\n","Epoch 97/200\n","782/782 [==============================] - 266s 340ms/step - loss: 0.0501 - accuracy: 0.9823 - val_loss: 0.4715 - val_accuracy: 0.9010\n","\n","Epoch 00097: loss did not improve from 0.04403\n","Epoch 98/200\n","782/782 [==============================] - 269s 344ms/step - loss: 0.0466 - accuracy: 0.9835 - val_loss: 0.5269 - val_accuracy: 0.8935\n","\n","Epoch 00098: loss did not improve from 0.04403\n","Epoch 99/200\n","782/782 [==============================] - 271s 347ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.5001 - val_accuracy: 0.9000\n","\n","Epoch 00099: loss did not improve from 0.04403\n","Epoch 100/200\n","782/782 [==============================] - 271s 347ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.5493 - val_accuracy: 0.8938\n","\n","Epoch 00100: loss did not improve from 0.04403\n","Epoch 101/200\n","782/782 [==============================] - 271s 347ms/step - loss: 0.0462 - accuracy: 0.9831 - val_loss: 0.4868 - val_accuracy: 0.8997\n","\n","Epoch 00101: loss did not improve from 0.04403\n","Epoch 102/200\n","782/782 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9837"]}],"source":["model.fit(iterator_train, batch_size=256, epochs=200, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["#### Continuing Training for 100 more Epochs (Total Epochs 20+102+100 = 222 Epochs)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","782/782 [==============================] - 315s 381ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.4888 - val_accuracy: 0.8949\n","\n","Epoch 00001: loss improved from inf to 0.04929, saving model to model.h5\n","Epoch 2/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0468 - accuracy: 0.9833 - val_loss: 0.4963 - val_accuracy: 0.8966\n","\n","Epoch 00002: loss improved from 0.04929 to 0.04675, saving model to model.h5\n","Epoch 3/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0526 - accuracy: 0.9821 - val_loss: 0.5100 - val_accuracy: 0.8910\n","\n","Epoch 00003: loss did not improve from 0.04675\n","Epoch 4/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.5261 - val_accuracy: 0.8936\n","\n","Epoch 00004: loss improved from 0.04675 to 0.04422, saving model to model.h5\n","Epoch 5/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0467 - accuracy: 0.9827 - val_loss: 0.5353 - val_accuracy: 0.8908\n","\n","Epoch 00005: loss did not improve from 0.04422\n","Epoch 6/100\n","782/782 [==============================] - 285s 365ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.5449 - val_accuracy: 0.8855\n","\n","Epoch 00006: loss did not improve from 0.04422\n","Epoch 7/100\n","782/782 [==============================] - 285s 365ms/step - loss: 0.0462 - accuracy: 0.9832 - val_loss: 0.5126 - val_accuracy: 0.8987\n","\n","Epoch 00007: loss did not improve from 0.04422\n","Epoch 8/100\n","782/782 [==============================] - 290s 371ms/step - loss: 0.0470 - accuracy: 0.9833 - val_loss: 0.5176 - val_accuracy: 0.8978\n","\n","Epoch 00008: loss did not improve from 0.04422\n","Epoch 9/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.5562 - val_accuracy: 0.8868\n","\n","Epoch 00009: loss did not improve from 0.04422\n","Epoch 10/100\n","782/782 [==============================] - 281s 360ms/step - loss: 0.0484 - accuracy: 0.9829 - val_loss: 0.5424 - val_accuracy: 0.8940\n","\n","Epoch 00010: loss did not improve from 0.04422\n","Epoch 11/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0466 - accuracy: 0.9839 - val_loss: 0.5023 - val_accuracy: 0.8993\n","\n","Epoch 00011: loss did not improve from 0.04422\n","Epoch 12/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0449 - accuracy: 0.9843 - val_loss: 0.5554 - val_accuracy: 0.8878\n","\n","Epoch 00012: loss did not improve from 0.04422\n","Epoch 13/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0439 - accuracy: 0.9841 - val_loss: 0.5243 - val_accuracy: 0.8904\n","\n","Epoch 00013: loss improved from 0.04422 to 0.04391, saving model to model.h5\n","Epoch 14/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0470 - accuracy: 0.9838 - val_loss: 0.4763 - val_accuracy: 0.8993\n","\n","Epoch 00014: loss did not improve from 0.04391\n","Epoch 15/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0458 - accuracy: 0.9835 - val_loss: 0.5739 - val_accuracy: 0.8861\n","\n","Epoch 00015: loss did not improve from 0.04391\n","Epoch 16/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0441 - accuracy: 0.9841 - val_loss: 0.4640 - val_accuracy: 0.9014\n","\n","Epoch 00016: loss did not improve from 0.04391\n","Epoch 17/100\n","782/782 [==============================] - 273s 349ms/step - loss: 0.0403 - accuracy: 0.9863 - val_loss: 0.5908 - val_accuracy: 0.8857\n","\n","Epoch 00017: loss improved from 0.04391 to 0.04027, saving model to model.h5\n","Epoch 18/100\n","782/782 [==============================] - 275s 352ms/step - loss: 0.0472 - accuracy: 0.9828 - val_loss: 0.5194 - val_accuracy: 0.8952\n","\n","Epoch 00018: loss did not improve from 0.04027\n","Epoch 19/100\n","782/782 [==============================] - 281s 359ms/step - loss: 0.0439 - accuracy: 0.9842 - val_loss: 0.5326 - val_accuracy: 0.8965\n","\n","Epoch 00019: loss did not improve from 0.04027\n","Epoch 20/100\n","782/782 [==============================] - 288s 369ms/step - loss: 0.0467 - accuracy: 0.9838 - val_loss: 0.5273 - val_accuracy: 0.8957\n","\n","Epoch 00020: loss did not improve from 0.04027\n","Epoch 21/100\n","782/782 [==============================] - 288s 368ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.4924 - val_accuracy: 0.8984\n","\n","Epoch 00021: loss did not improve from 0.04027\n","Epoch 22/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.4770 - val_accuracy: 0.8997\n","\n","Epoch 00022: loss did not improve from 0.04027\n","Epoch 23/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0399 - accuracy: 0.9855 - val_loss: 0.4870 - val_accuracy: 0.9007\n","\n","Epoch 00023: loss improved from 0.04027 to 0.03988, saving model to model.h5\n","Epoch 24/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0432 - accuracy: 0.9847 - val_loss: 0.5831 - val_accuracy: 0.8863\n","\n","Epoch 00024: loss did not improve from 0.03988\n","Epoch 25/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0424 - accuracy: 0.9849 - val_loss: 0.5288 - val_accuracy: 0.8949\n","\n","Epoch 00025: loss did not improve from 0.03988\n","Epoch 26/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0412 - accuracy: 0.9853 - val_loss: 0.5266 - val_accuracy: 0.9000\n","\n","Epoch 00026: loss did not improve from 0.03988\n","Epoch 27/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0407 - accuracy: 0.9853 - val_loss: 0.5601 - val_accuracy: 0.8884\n","\n","Epoch 00027: loss did not improve from 0.03988\n","Epoch 28/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 0.5728 - val_accuracy: 0.8937\n","\n","Epoch 00028: loss did not improve from 0.03988\n","Epoch 29/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0419 - accuracy: 0.9847 - val_loss: 0.5208 - val_accuracy: 0.8986\n","\n","Epoch 00029: loss did not improve from 0.03988\n","Epoch 30/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0408 - accuracy: 0.9855 - val_loss: 0.5056 - val_accuracy: 0.8974\n","\n","Epoch 00030: loss did not improve from 0.03988\n","Epoch 31/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0400 - accuracy: 0.9856 - val_loss: 0.4928 - val_accuracy: 0.8989\n","\n","Epoch 00031: loss did not improve from 0.03988\n","Epoch 32/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0432 - accuracy: 0.9852 - val_loss: 0.4501 - val_accuracy: 0.9027\n","\n","Epoch 00032: loss did not improve from 0.03988\n","Epoch 33/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0373 - accuracy: 0.9863 - val_loss: 0.4979 - val_accuracy: 0.8967\n","\n","Epoch 00033: loss improved from 0.03988 to 0.03728, saving model to model.h5\n","Epoch 34/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 0.5154 - val_accuracy: 0.8992\n","\n","Epoch 00034: loss did not improve from 0.03728\n","Epoch 35/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0400 - accuracy: 0.9857 - val_loss: 0.5521 - val_accuracy: 0.8966\n","\n","Epoch 00035: loss did not improve from 0.03728\n","Epoch 36/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0407 - accuracy: 0.9858 - val_loss: 0.5471 - val_accuracy: 0.8920\n","\n","Epoch 00036: loss did not improve from 0.03728\n","Epoch 37/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0405 - accuracy: 0.9858 - val_loss: 0.5380 - val_accuracy: 0.8952\n","\n","Epoch 00037: loss did not improve from 0.03728\n","Epoch 38/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0379 - accuracy: 0.9863 - val_loss: 0.5319 - val_accuracy: 0.8949\n","\n","Epoch 00038: loss did not improve from 0.03728\n","Epoch 39/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0381 - accuracy: 0.9864 - val_loss: 0.5723 - val_accuracy: 0.8962\n","\n","Epoch 00039: loss did not improve from 0.03728\n","Epoch 40/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0391 - accuracy: 0.9864 - val_loss: 0.5101 - val_accuracy: 0.8993\n","\n","Epoch 00040: loss did not improve from 0.03728\n","Epoch 41/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0380 - accuracy: 0.9861 - val_loss: 0.4959 - val_accuracy: 0.9023\n","\n","Epoch 00041: loss did not improve from 0.03728\n","Epoch 42/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0407 - accuracy: 0.9854 - val_loss: 0.4854 - val_accuracy: 0.9053\n","\n","Epoch 00042: loss did not improve from 0.03728\n","Epoch 43/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0377 - accuracy: 0.9859 - val_loss: 0.5266 - val_accuracy: 0.8966\n","\n","Epoch 00043: loss did not improve from 0.03728\n","Epoch 44/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 0.4973 - val_accuracy: 0.9069\n","\n","Epoch 00044: loss did not improve from 0.03728\n","Epoch 45/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0353 - accuracy: 0.9877 - val_loss: 0.5549 - val_accuracy: 0.8962\n","\n","Epoch 00045: loss improved from 0.03728 to 0.03526, saving model to model.h5\n","Epoch 46/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 0.5128 - val_accuracy: 0.9018\n","\n","Epoch 00046: loss did not improve from 0.03526\n","Epoch 47/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.6678 - val_accuracy: 0.8834\n","\n","Epoch 00047: loss did not improve from 0.03526\n","Epoch 48/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0376 - accuracy: 0.9867 - val_loss: 0.5471 - val_accuracy: 0.8920\n","\n","Epoch 00048: loss did not improve from 0.03526\n","Epoch 49/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0361 - accuracy: 0.9872 - val_loss: 0.5838 - val_accuracy: 0.8893\n","\n","Epoch 00049: loss did not improve from 0.03526\n","Epoch 50/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0370 - accuracy: 0.9872 - val_loss: 0.5726 - val_accuracy: 0.8923\n","\n","Epoch 00050: loss did not improve from 0.03526\n","Epoch 51/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0365 - accuracy: 0.9870 - val_loss: 0.5073 - val_accuracy: 0.9047\n","\n","Epoch 00051: loss did not improve from 0.03526\n","Epoch 52/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0370 - accuracy: 0.9872 - val_loss: 0.4766 - val_accuracy: 0.9037\n","\n","Epoch 00052: loss did not improve from 0.03526\n","Epoch 53/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0363 - accuracy: 0.9874 - val_loss: 0.5285 - val_accuracy: 0.8989\n","\n","Epoch 00053: loss did not improve from 0.03526\n","Epoch 54/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0371 - accuracy: 0.9869 - val_loss: 0.5193 - val_accuracy: 0.8955\n","\n","Epoch 00054: loss did not improve from 0.03526\n","Epoch 55/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.5117 - val_accuracy: 0.9003\n","\n","Epoch 00055: loss did not improve from 0.03526\n","Epoch 56/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0359 - accuracy: 0.9873 - val_loss: 0.5007 - val_accuracy: 0.8971\n","\n","Epoch 00056: loss did not improve from 0.03526\n","Epoch 57/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0363 - accuracy: 0.9873 - val_loss: 0.5429 - val_accuracy: 0.8978\n","\n","Epoch 00057: loss did not improve from 0.03526\n","Epoch 58/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0351 - accuracy: 0.9877 - val_loss: 0.5240 - val_accuracy: 0.8942\n","\n","Epoch 00058: loss improved from 0.03526 to 0.03507, saving model to model.h5\n","Epoch 59/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0391 - accuracy: 0.9862 - val_loss: 0.4751 - val_accuracy: 0.9064\n","\n","Epoch 00059: loss did not improve from 0.03507\n","Epoch 60/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.5224 - val_accuracy: 0.9002\n","\n","Epoch 00060: loss improved from 0.03507 to 0.03424, saving model to model.h5\n","Epoch 61/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0348 - accuracy: 0.9881 - val_loss: 0.5519 - val_accuracy: 0.8962\n","\n","Epoch 00061: loss did not improve from 0.03424\n","Epoch 62/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0376 - accuracy: 0.9868 - val_loss: 0.5635 - val_accuracy: 0.8992\n","\n","Epoch 00062: loss did not improve from 0.03424\n","Epoch 63/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.5721 - val_accuracy: 0.8942\n","\n","Epoch 00063: loss improved from 0.03424 to 0.03390, saving model to model.h5\n","Epoch 64/100\n","782/782 [==============================] - 284s 363ms/step - loss: 0.0343 - accuracy: 0.9875 - val_loss: 0.5737 - val_accuracy: 0.8945\n","\n","Epoch 00064: loss did not improve from 0.03390\n","Epoch 65/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0347 - accuracy: 0.9878 - val_loss: 0.5657 - val_accuracy: 0.8956\n","\n","Epoch 00065: loss did not improve from 0.03390\n","Epoch 66/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0333 - accuracy: 0.9879 - val_loss: 0.5559 - val_accuracy: 0.8965\n","\n","Epoch 00066: loss improved from 0.03390 to 0.03335, saving model to model.h5\n","Epoch 67/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0348 - accuracy: 0.9871 - val_loss: 0.5079 - val_accuracy: 0.8998\n","\n","Epoch 00067: loss did not improve from 0.03335\n","Epoch 68/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0358 - accuracy: 0.9872 - val_loss: 0.5230 - val_accuracy: 0.9017\n","\n","Epoch 00068: loss did not improve from 0.03335\n","Epoch 69/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0316 - accuracy: 0.9887 - val_loss: 0.5206 - val_accuracy: 0.8999\n","\n","Epoch 00069: loss improved from 0.03335 to 0.03157, saving model to model.h5\n","Epoch 70/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0337 - accuracy: 0.9882 - val_loss: 0.5224 - val_accuracy: 0.9018\n","\n","Epoch 00070: loss did not improve from 0.03157\n","Epoch 71/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0344 - accuracy: 0.9875 - val_loss: 0.5391 - val_accuracy: 0.8964\n","\n","Epoch 00071: loss did not improve from 0.03157\n","Epoch 72/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0316 - accuracy: 0.9889 - val_loss: 0.5493 - val_accuracy: 0.8960\n","\n","Epoch 00072: loss improved from 0.03157 to 0.03156, saving model to model.h5\n","Epoch 73/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0325 - accuracy: 0.9884 - val_loss: 0.5775 - val_accuracy: 0.8991\n","\n","Epoch 00073: loss did not improve from 0.03156\n","Epoch 74/100\n","782/782 [==============================] - 285s 364ms/step - loss: 0.0373 - accuracy: 0.9866 - val_loss: 0.5417 - val_accuracy: 0.8984\n","\n","Epoch 00074: loss did not improve from 0.03156\n","Epoch 75/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0308 - accuracy: 0.9890 - val_loss: 0.5240 - val_accuracy: 0.8995\n","\n","Epoch 00075: loss improved from 0.03156 to 0.03085, saving model to model.h5\n","Epoch 76/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0355 - accuracy: 0.9873 - val_loss: 0.5810 - val_accuracy: 0.8896\n","\n","Epoch 00076: loss did not improve from 0.03085\n","Epoch 77/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0337 - accuracy: 0.9875 - val_loss: 0.5299 - val_accuracy: 0.8973\n","\n","Epoch 00077: loss did not improve from 0.03085\n","Epoch 78/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0341 - accuracy: 0.9881 - val_loss: 0.5268 - val_accuracy: 0.9002\n","\n","Epoch 00078: loss did not improve from 0.03085\n","Epoch 79/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0346 - accuracy: 0.9878 - val_loss: 0.5407 - val_accuracy: 0.8975\n","\n","Epoch 00079: loss did not improve from 0.03085\n","Epoch 80/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0327 - accuracy: 0.9887 - val_loss: 0.5777 - val_accuracy: 0.8978\n","\n","Epoch 00080: loss did not improve from 0.03085\n","Epoch 81/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0310 - accuracy: 0.9887 - val_loss: 0.5592 - val_accuracy: 0.8929\n","\n","Epoch 00081: loss did not improve from 0.03085\n","Epoch 82/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0312 - accuracy: 0.9889 - val_loss: 0.6145 - val_accuracy: 0.8935\n","\n","Epoch 00082: loss did not improve from 0.03085\n","Epoch 83/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0340 - accuracy: 0.9884 - val_loss: 0.5081 - val_accuracy: 0.9045\n","\n","Epoch 00083: loss did not improve from 0.03085\n","Epoch 84/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0331 - accuracy: 0.9881 - val_loss: 0.5290 - val_accuracy: 0.9059\n","\n","Epoch 00084: loss did not improve from 0.03085\n","Epoch 85/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0306 - accuracy: 0.9894 - val_loss: 0.5033 - val_accuracy: 0.9025\n","\n","Epoch 00085: loss improved from 0.03085 to 0.03058, saving model to model.h5\n","Epoch 86/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0327 - accuracy: 0.9885 - val_loss: 0.5267 - val_accuracy: 0.8998\n","\n","Epoch 00086: loss did not improve from 0.03058\n","Epoch 87/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0326 - accuracy: 0.9885 - val_loss: 0.5396 - val_accuracy: 0.8976\n","\n","Epoch 00087: loss did not improve from 0.03058\n","Epoch 88/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0312 - accuracy: 0.9889 - val_loss: 0.5423 - val_accuracy: 0.9008\n","\n","Epoch 00088: loss did not improve from 0.03058\n","Epoch 89/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 0.5230 - val_accuracy: 0.9069\n","\n","Epoch 00089: loss improved from 0.03058 to 0.02970, saving model to model.h5\n","Epoch 90/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0322 - accuracy: 0.9887 - val_loss: 0.6222 - val_accuracy: 0.8926\n","\n","Epoch 00090: loss did not improve from 0.02970\n","Epoch 91/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.6205 - val_accuracy: 0.8920\n","\n","Epoch 00091: loss did not improve from 0.02970\n","Epoch 92/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0314 - accuracy: 0.9888 - val_loss: 0.5309 - val_accuracy: 0.9021\n","\n","Epoch 00092: loss did not improve from 0.02970\n","Epoch 93/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.5156 - val_accuracy: 0.9024\n","\n","Epoch 00093: loss did not improve from 0.02970\n","Epoch 94/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.5367 - val_accuracy: 0.9020\n","\n","Epoch 00094: loss did not improve from 0.02970\n","Epoch 95/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0322 - accuracy: 0.9887 - val_loss: 0.5090 - val_accuracy: 0.9037\n","\n","Epoch 00095: loss did not improve from 0.02970\n","Epoch 96/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.5692 - val_accuracy: 0.8952\n","\n","Epoch 00096: loss did not improve from 0.02970\n","Epoch 97/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0307 - accuracy: 0.9893 - val_loss: 0.5376 - val_accuracy: 0.9021\n","\n","Epoch 00097: loss did not improve from 0.02970\n","Epoch 98/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0315 - accuracy: 0.9895 - val_loss: 0.5633 - val_accuracy: 0.8974\n","\n","Epoch 00098: loss did not improve from 0.02970\n","Epoch 99/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0298 - accuracy: 0.9894 - val_loss: 0.5081 - val_accuracy: 0.9048\n","\n","Epoch 00099: loss did not improve from 0.02970\n","Epoch 100/100\n","782/782 [==============================] - 284s 364ms/step - loss: 0.0329 - accuracy: 0.9884 - val_loss: 0.5514 - val_accuracy: 0.9003\n","\n","Epoch 00100: loss did not improve from 0.02970\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1d03b5c6f40>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(iterator_train, batch_size=256, epochs=100, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 21s 58ms/step - loss: 0.5514 - accuracy: 0.9003\n","Test loss: 0.551444947719574\n","Test accuracy: 0.9003000259399414\n"]}],"source":["# Test the model\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{},"source":["# After Training model (222 Epochs) got the Test Accuracy of __90.03%.__"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CNN on CIFR Assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
